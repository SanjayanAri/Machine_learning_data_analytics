{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12853160,"sourceType":"datasetVersion","datasetId":8129551}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sanjayana/medical-insurance-analysis?scriptVersionId=261469926\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Medical insurance trends analysis\n\nAuthor: Sanjayan.A\n\nAbout the dataset: This dataset contains medical insurance cost information for 1338 individuals. It includes demographic and health-related variables such as age, sex, BMI, number of children, smoking status, and residential region in the US. The target variable is charges, which represents the medical insurance cost billed to the individual.\n\nDataset source citation: mosap abdelghany. (2025). Medical Insurance Cost Dataset [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DSV/12853160\n\nSo in this notebook I am going to analyze the dataset and then do some small prediction ML models like linear regression, Random forest, XGBoost, to predict the insurance price. I am learning to do the Machine learning models through this dataset.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib\nmatplotlib.use('Agg')  \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n# Set aesthetic parameters for seaborn\nsns.set(style=\"whitegrid\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T16:37:26.11023Z","iopub.execute_input":"2025-09-12T16:37:26.110436Z","iopub.status.idle":"2025-09-12T16:37:30.195748Z","shell.execute_reply.started":"2025-09-12T16:37:26.110416Z","shell.execute_reply":"2025-09-12T16:37:30.194885Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Importing the data","metadata":{}},{"cell_type":"code","source":"# Load the dataset\ndata_path = '/kaggle/input/medical-insurance-cost-dataset/insurance.csv'\ndf = pd.read_csv(data_path, encoding='ascii', delimiter=',')\n\n# Display the first few rows to check the dataset structure\nprint('Dataset loaded')\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T16:39:22.820805Z","iopub.execute_input":"2025-09-12T16:39:22.821135Z","iopub.status.idle":"2025-09-12T16:39:22.855392Z","shell.execute_reply.started":"2025-09-12T16:39:22.82111Z","shell.execute_reply":"2025-09-12T16:39:22.854586Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Cleaning the data for removing undesirables","metadata":{}},{"cell_type":"code","source":"# Checking for null values\nnull_counts = df.isnull().sum()\nprint('Null values in each column:')\nprint(null_counts)\n\n# Display the data types of the columns\nprint('\\nData types:')\nprint(df.dtypes)\n\n\ncategorical_cols = ['sex', 'smoker', 'region']\n\n# Verify if there are any extra whitespaces or inconsistencies\nfor col in categorical_cols:\n    df[col] = df[col].str.strip()\n\nprint('\\nCategorical columns cleaned successfully.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T16:41:18.830121Z","iopub.execute_input":"2025-09-12T16:41:18.830525Z","iopub.status.idle":"2025-09-12T16:41:18.847297Z","shell.execute_reply.started":"2025-09-12T16:41:18.830495Z","shell.execute_reply":"2025-09-12T16:41:18.846211Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The dataset is cleaned so lets proceed with the exploration.","metadata":{}},{"cell_type":"code","source":"# Age distribution plot\n\nplt.figure(figsize=(14, 10))\nplt.subplot(2, 2, 1)\nsns.histplot(df['age'], kde=True, color='blue')\nplt.title('Age Distribution')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T16:43:45.037886Z","iopub.execute_input":"2025-09-12T16:43:45.03828Z","iopub.status.idle":"2025-09-12T16:43:45.587776Z","shell.execute_reply.started":"2025-09-12T16:43:45.038256Z","shell.execute_reply":"2025-09-12T16:43:45.586842Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"we can observe a lot of youngsters here.","metadata":{}},{"cell_type":"code","source":"#BMI chart\nplt.figure(figsize=(8, 8))\nsns.histplot(df['bmi'], kde=True, color='green')\nplt.title('BMI Distribution')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T16:47:34.684966Z","iopub.execute_input":"2025-09-12T16:47:34.685327Z","iopub.status.idle":"2025-09-12T16:47:35.114084Z","shell.execute_reply.started":"2025-09-12T16:47:34.685302Z","shell.execute_reply":"2025-09-12T16:47:35.113009Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"BMI of 30 is observed to be present in higher numbers.","metadata":{}},{"cell_type":"code","source":"# Insurance charges\nplt.figure(figsize=(10, 8))\nsns.histplot(df['charges'], kde=True, color='red')\nplt.title('Insurance Charges Distribution')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T16:48:45.194068Z","iopub.execute_input":"2025-09-12T16:48:45.194402Z","iopub.status.idle":"2025-09-12T16:48:45.595772Z","shell.execute_reply.started":"2025-09-12T16:48:45.194382Z","shell.execute_reply":"2025-09-12T16:48:45.594743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# distribution of children in this dataset\nplt.figure(figsize=(10, 8))\nsns.countplot(x='children', data=df, palette='viridis')\nplt.title('Number of Children')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T16:49:32.51327Z","iopub.execute_input":"2025-09-12T16:49:32.513634Z","iopub.status.idle":"2025-09-12T16:49:32.779254Z","shell.execute_reply.started":"2025-09-12T16:49:32.513608Z","shell.execute_reply":"2025-09-12T16:49:32.778322Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Most people have 0-3 children, only few have 4 or 5 children.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n# pair plot\nsns.pairplot(df, hue='smoker')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T16:51:12.731487Z","iopub.execute_input":"2025-09-12T16:51:12.732201Z","iopub.status.idle":"2025-09-12T16:51:18.520692Z","shell.execute_reply.started":"2025-09-12T16:51:12.732167Z","shell.execute_reply":"2025-09-12T16:51:18.519725Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## This pair plot says\nBMI is being around 30, the age of the people lies from 20 to 60, most familes have 0-3 children, few have 4 or 5 and charges most people have less charge, few pay more, particularly as the age increase the charges increases too. \n\nFor the smokers they are charged more than the non-smokers, and BMI is higher in the smokers. \n\nNumber of Childern does not influence the insurance charges. ","metadata":{}},{"cell_type":"code","source":"# Box plot of charges vs. smoker status\nplt.figure(figsize=(8, 6))\nsns.boxplot(x='smoker', y='charges', data=df, palette='Set2')\nplt.title('Insurance Charges vs. Smoker Status')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T16:54:52.20304Z","iopub.execute_input":"2025-09-12T16:54:52.203395Z","iopub.status.idle":"2025-09-12T16:54:52.383655Z","shell.execute_reply.started":"2025-09-12T16:54:52.203367Z","shell.execute_reply":"2025-09-12T16:54:52.382717Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Smokers pay more amount than the non smokers, smoking is the driving factor for the insurance charges. ","metadata":{}},{"cell_type":"code","source":"numeric_df = df.select_dtypes(include=[np.number])\nif numeric_df.shape[1] >= 4:\n    plt.figure(figsize=(8,6))\n    corr = numeric_df.corr()\n    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n    plt.title('Correlation Heatmap')\n    plt.show()\nelse:\n    print('Not enough numeric columns for a correlation heatmap.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T16:56:24.901854Z","iopub.execute_input":"2025-09-12T16:56:24.902244Z","iopub.status.idle":"2025-09-12T16:56:25.168849Z","shell.execute_reply.started":"2025-09-12T16:56:24.902216Z","shell.execute_reply":"2025-09-12T16:56:25.167784Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Correlation\n\nthere is no correlation between children and charges (0.07)\nthere is little correlation between BMI and charges (0.2) and age (0.3)\n","metadata":{}},{"cell_type":"code","source":"#Insurance charges by humans\nplt.figure(figsize=(8,5))\nsns.boxplot(x=\"region\", y=\"charges\", data=df, palette=\"Set3\")\nplt.title(\"Insurance Charges by Region\", fontsize=14)\nplt.xlabel(\"Region\")\nplt.ylabel(\"Charges\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T16:57:27.335003Z","iopub.execute_input":"2025-09-12T16:57:27.335381Z","iopub.status.idle":"2025-09-12T16:57:27.546697Z","shell.execute_reply.started":"2025-09-12T16:57:27.33535Z","shell.execute_reply":"2025-09-12T16:57:27.545686Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Regression model\n\nWe are going to build a model to predict an insurance cost.\n\nThe model tries to fit a line like this:\n\ny=mx+b\n\n\ny = the thing you want to predict (target, e.g, charges)\n\nx = the input feature (e.g, age, BMI, smoker)\n\nm = slope (how much y changes when x increases)\n\nb = intercept (where the line starts when x = 0)","metadata":{}},{"cell_type":"code","source":"# Creating dummy variables for categorical features\ndf_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n\n# Define feature matrix X and target variable y\nX = df_encoded.drop('charges', axis=1)\ny = df_encoded['charges']\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train the Linear Regression model\nlr_model = LinearRegression()\nlr_model.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = lr_model.predict(X_test)\n\n# Calculate the R-squared score\nr2 = r2_score(y_test, y_pred)\nprint('Linear Regression R-squared score:', r2)\n\n# For a quick look, plot actual vs predicted charges\nplt.figure(figsize=(8,6))\nplt.scatter(y_test, y_pred, alpha=0.7, color='purple')\nplt.xlabel('Actual Charges')\nplt.ylabel('Predicted Charges')\nplt.title('Actual vs Predicted Insurance Charges')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linewidth=2)  # Line for perfect prediction\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T17:00:57.848787Z","iopub.execute_input":"2025-09-12T17:00:57.850069Z","iopub.status.idle":"2025-09-12T17:00:58.133241Z","shell.execute_reply.started":"2025-09-12T17:00:57.850031Z","shell.execute_reply":"2025-09-12T17:00:58.132223Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## What happened here\n\nFirst we are converting the data to the variables which computer can understand, such as 1 and 0 binary. So for e.g smoker = 1 non-smoker = 0 like that. \n\nThen we are picking the input and the target in Y which is insurance cost, x is the information we have like age, sex etc\n\nThen we are teaching the model by spliting to two training data (for machine to understand) and testing data (for it to practice/test).\n\nNow we are making the model to do the linear regression with the data it learned from, and the model makes the prediction from the test dataset it has, and gives scores based on it. \n\nThe graph is plotted by the model, the purple dots mark each person and the red line is the prefect guess. More the dots close to the red line better the prediction. \n\n### Linear Regression R-squared score: 0.7835929767120724\n\nSo 0 means model is throwing random guesses and 1 means model is giving the most accurate prediction\n\nour model here built lies in ~ 0.78, explaining about 78% of the variance in the cost. ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error\nimport numpy as np\n\nmae = mean_absolute_error(y_test, y_pred)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\n\nprint(\"MAE:\", mae)\nprint(\"RMSE:\", rmse)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T17:49:55.121086Z","iopub.execute_input":"2025-09-12T17:49:55.123867Z","iopub.status.idle":"2025-09-12T17:49:55.1452Z","shell.execute_reply.started":"2025-09-12T17:49:55.123831Z","shell.execute_reply":"2025-09-12T17:49:55.144178Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The MAE (mean absolute error) and the RMSE (Root Mean Square Error) says our prediction are off by 4181 and 5796 dollars, which can be due to the outliers in the data and here the smoking is the huge influencer in the charges. ","metadata":{}},{"cell_type":"markdown","source":"# Trying different model for prediction\nSince we Know that Linear regression model have a huge error difference we are trying to build other models like RandomForest and XG booster. ","metadata":{}},{"cell_type":"markdown","source":"## Building RandomForestRegressor \n\nThis is a ensemble based learning method which combines multiple models to improve accuracy, this makes decision trees by bootstrap sampling, construction of trees where each tree is trained on its subset. At each split in the tree, only a random subset of features is considered","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport numpy as np\n\n# Train Random Forest\nrf_model = RandomForestRegressor(n_estimators=200, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# Predictions\ny_pred_rf = rf_model.predict(X_test)\n\n# Evaluation\nprint(\"Random Forest R²:\", r2_score(y_test, y_pred_rf))\nprint(\"MAE:\", mean_absolute_error(y_test, y_pred_rf))\nprint(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_rf)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T17:58:08.180486Z","iopub.execute_input":"2025-09-12T17:58:08.18085Z","iopub.status.idle":"2025-09-12T17:58:09.132141Z","shell.execute_reply.started":"2025-09-12T17:58:08.180827Z","shell.execute_reply":"2025-09-12T17:58:09.131301Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## XGBoost Model\n\nXGBoost is a boosting algorithm, another type of ensemble learning. Unlike Random Forest, boosting builds trees sequentially, where each new tree tries to correct the errors of previous trees. \n\nThis works by starting with a simple prediction, then compute residuals (errors) of the current model, after which it trains a new tree to predict these residuals. Then this combine predictions of all trees (weighted sum) to get the final output, these are repeated for a number of iterations (trees).","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\n# Train XGBoost\nxgb_model = XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6, random_state=42)\nxgb_model.fit(X_train, y_train)\n\n# Predictions\ny_pred_xgb = xgb_model.predict(X_test)\n\n# Evaluation\nprint(\"XGBoost R²:\", r2_score(y_test, y_pred_xgb))\nprint(\"MAE:\", mean_absolute_error(y_test, y_pred_xgb))\nprint(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_xgb)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T17:58:33.557571Z","iopub.execute_input":"2025-09-12T17:58:33.558238Z","iopub.status.idle":"2025-09-12T17:58:34.372771Z","shell.execute_reply.started":"2025-09-12T17:58:33.558215Z","shell.execute_reply":"2025-09-12T17:58:34.37135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Comparing the models\nresults = {\n    \"Linear Regression\": [r2_score(y_test, lr_model.predict(X_test)),\n                          mean_absolute_error(y_test, lr_model.predict(X_test)),\n                          np.sqrt(mean_squared_error(y_test, lr_model.predict(X_test)))],\n    \n    \"Random Forest\": [r2_score(y_test, y_pred_rf),\n                      mean_absolute_error(y_test, y_pred_rf),\n                      np.sqrt(mean_squared_error(y_test, y_pred_rf))],\n    \n    \"XGBoost\": [r2_score(y_test, y_pred_xgb),\n                mean_absolute_error(y_test, y_pred_xgb),\n                np.sqrt(mean_squared_error(y_test, y_pred_xgb))]\n}\n\ndf_results = pd.DataFrame(results, index=[\"R²\", \"MAE\", \"RMSE\"])\nprint(df_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T18:03:16.389881Z","iopub.execute_input":"2025-09-12T18:03:16.39083Z","iopub.status.idle":"2025-09-12T18:03:16.418485Z","shell.execute_reply.started":"2025-09-12T18:03:16.390794Z","shell.execute_reply":"2025-09-12T18:03:16.41729Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As we can compare here that the Random forest and XGBoost have better R^2 values and the lower MAE and RMSE values than the linear regression model.","metadata":{}},{"cell_type":"markdown","source":"## Tuning the Models","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\n\n# Define the parameter grid\nparam_dist = {\n    'n_estimators': [100, 200, 300, 500],\n    'max_depth': [None, 5, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['auto', 'sqrt']\n}\n\n# Initialize model\nrf = RandomForestRegressor(random_state=42)\n\n# Randomized Search\nrf_random = RandomizedSearchCV(estimator=rf,\n                               param_distributions=param_dist,\n                               n_iter=20,  \n                               cv=5,\n                               scoring='r2',\n                               random_state=42,\n                               n_jobs=-1,\n                               verbose=2)\n\nrf_random.fit(X_train, y_train)\n\n# Best parameters\nprint(\"Best Random Forest params:\", rf_random.best_params_)\n\n# Evaluate best model\nbest_rf = rf_random.best_estimator_\ny_pred_best_rf = best_rf.predict(X_test)\n\nprint(\"Best RF R²:\", r2_score(y_test, y_pred_best_rf))\nprint(\"Best RF MAE:\", mean_absolute_error(y_test, y_pred_best_rf))\nprint(\"Best RF RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_best_rf)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T18:10:06.462572Z","iopub.execute_input":"2025-09-12T18:10:06.463425Z","iopub.status.idle":"2025-09-12T18:10:27.58018Z","shell.execute_reply.started":"2025-09-12T18:10:06.463395Z","shell.execute_reply":"2025-09-12T18:10:27.579231Z"},"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As we see there is improvement in the model after hypertuning.\n\nBest RF R²: 0.8737764527105805\nBest RF MAE: 2554.493417384744\nBest RF RMSE: 4426.742097872235","metadata":{}},{"cell_type":"markdown","source":"## Tuning the XGBoost model","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom xgboost import XGBRegressor\n\n# Define parameter grid\nparam_dist_xgb = {\n    'n_estimators': [200, 500, 800],\n    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n    'max_depth': [3, 5, 7, 10],\n    'subsample': [0.6, 0.8, 1.0],\n    'colsample_bytree': [0.6, 0.8, 1.0]\n}\n\n# Initialize\nxgb = XGBRegressor(random_state=42)\n\n# Randomized search\nxgb_random = RandomizedSearchCV(estimator=xgb,\n                                param_distributions=param_dist_xgb,\n                                n_iter=20,\n                                cv=5,\n                                scoring='r2',\n                                random_state=42,\n                                n_jobs=-1,\n                                verbose=2)\n\nxgb_random.fit(X_train, y_train)\n\n# Best parameters\nprint(\"Best XGBoost params:\", xgb_random.best_params_)\n\n# Evaluate best model\nbest_xgb = xgb_random.best_estimator_\ny_pred_best_xgb = best_xgb.predict(X_test)\n\nprint(\"Best XGB R²:\", r2_score(y_test, y_pred_best_xgb))\nprint(\"Best XGB MAE:\", mean_absolute_error(y_test, y_pred_best_xgb))\nprint(\"Best XGB RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_best_xgb)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T18:11:28.295956Z","iopub.execute_input":"2025-09-12T18:11:28.296342Z","iopub.status.idle":"2025-09-12T18:11:40.398103Z","shell.execute_reply.started":"2025-09-12T18:11:28.296314Z","shell.execute_reply":"2025-09-12T18:11:40.397218Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"we also see an improvement in the model after tuning them \n\nBest XGB R²: 0.8798444442039779\nBest XGB MAE: 2545.9320036401587\nBest XGB RMSE: 4319.027392714875\n","metadata":{}},{"cell_type":"code","source":"# Comparing both tuned model\nresults_tuned = {\n    \"Random Forest (Tuned)\": [r2_score(y_test, y_pred_best_rf),\n                              mean_absolute_error(y_test, y_pred_best_rf),\n                              np.sqrt(mean_squared_error(y_test, y_pred_best_rf))],\n    \n    \"XGBoost (Tuned)\": [r2_score(y_test, y_pred_best_xgb),\n                        mean_absolute_error(y_test, y_pred_best_xgb),\n                        np.sqrt(mean_squared_error(y_test, y_pred_best_xgb))]\n}\n\ndf_results_tuned = pd.DataFrame(results_tuned, index=[\"R²\", \"MAE\", \"RMSE\"])\nprint(df_results_tuned)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T18:16:55.780026Z","iopub.execute_input":"2025-09-12T18:16:55.780383Z","iopub.status.idle":"2025-09-12T18:16:55.794984Z","shell.execute_reply.started":"2025-09-12T18:16:55.780357Z","shell.execute_reply":"2025-09-12T18:16:55.793932Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Importance ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Get feature importance\nimportances = best_rf.feature_importances_\nfeatures = X_train.columns\n\n# Put into DataFrame\nrf_importance_df = pd.DataFrame({\n    \"Feature\": features,\n    \"Importance\": importances\n}).sort_values(by=\"Importance\", ascending=False)\n\n# Plot\nplt.figure(figsize=(10,6))\nsns.barplot(x=\"Importance\", y=\"Feature\", data=rf_importance_df.head(15), palette=\"viridis\")\nplt.title(\"Random Forest - Top 15 Feature Importances\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T18:17:15.485479Z","iopub.execute_input":"2025-09-12T18:17:15.486492Z","iopub.status.idle":"2025-09-12T18:17:15.776827Z","shell.execute_reply.started":"2025-09-12T18:17:15.486455Z","shell.execute_reply":"2025-09-12T18:17:15.775574Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For XG Boost\n\n\nimportances_xgb = best_xgb.feature_importances_\nfeatures = X_train.columns\n\n# Put into DataFrame\nxgb_importance_df = pd.DataFrame({\n    \"Feature\": features,\n    \"Importance\": importances_xgb\n}).sort_values(by=\"Importance\", ascending=False)\n\n# Plot\nplt.figure(figsize=(10,6))\nsns.barplot(x=\"Importance\", y=\"Feature\", data=xgb_importance_df.head(15), palette=\"magma\")\nplt.title(\"XGBoost - Top 15 Feature Importances\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T18:17:58.294716Z","iopub.execute_input":"2025-09-12T18:17:58.295239Z","iopub.status.idle":"2025-09-12T18:17:58.627358Z","shell.execute_reply.started":"2025-09-12T18:17:58.295134Z","shell.execute_reply":"2025-09-12T18:17:58.626457Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Which is stronger \nAs we compare here witht the feature importance for both the model, XGBoost seems to include lot more factors on prediction and the R^2 value is more stronger in the XGBoost than the linear regression model or the Random forest model after tuning.","metadata":{}},{"cell_type":"code","source":"import shap\n\n# Create SHAP explainer\nexplainer = shap.TreeExplainer(best_xgb)   # best_xgb is your tuned XGBoost model\nshap_values = explainer.shap_values(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T18:21:23.644023Z","iopub.execute_input":"2025-09-12T18:21:23.644404Z","iopub.status.idle":"2025-09-12T18:21:32.348273Z","shell.execute_reply.started":"2025-09-12T18:21:23.644381Z","shell.execute_reply":"2025-09-12T18:21:32.34677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Global summary (all features across all samples)\nshap.summary_plot(shap_values, X_test)\n\n# Bar version (easier to read)\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T18:24:39.35465Z","iopub.execute_input":"2025-09-12T18:24:39.355096Z","iopub.status.idle":"2025-09-12T18:24:39.832782Z","shell.execute_reply.started":"2025-09-12T18:24:39.355069Z","shell.execute_reply":"2025-09-12T18:24:39.831611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Explain one prediction\ni = 0  # choose row index from X_test\nforce_plot = shap.force_plot(explainer.expected_value, shap_values[i,:], X_test.iloc[i,:])\n\n# Save to HTML\nshap.save_html(\"force_plot_example.html\", force_plot)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T18:25:20.239392Z","iopub.execute_input":"2025-09-12T18:25:20.239775Z","iopub.status.idle":"2025-09-12T18:25:20.250824Z","shell.execute_reply.started":"2025-09-12T18:25:20.239753Z","shell.execute_reply":"2025-09-12T18:25:20.249634Z"}},"outputs":[],"execution_count":null}]}